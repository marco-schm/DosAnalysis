{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Reduction\n",
    "## Overview\n",
    "This notebook processes a very large intrusion-detection dataset that contains various types of network attacks, including DDoS, DoS, brute-force attempts, SQL injection, and more. Because the full dataset is over 7 GB in size and includes several million network packets, training complex models directly on all data is impractical without specialized hardware. The goal of this notebook is therefore to reduce the dataset in a targeted way and focus exclusively on DoS and DDoS attacks, as these categories are both sufficiently represented and highly relevant for security research. The following documentation explains step by step how this reduction is implemented across the individual code cells."
   ],
   "id": "e2d3c3c7e5705103"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T03:19:52.666580Z",
     "start_time": "2025-11-21T03:19:52.663227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import os\n",
    "import kagglehub\n"
   ],
   "id": "fb942ff5bb07c4a7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Loading Strategy\n",
    "Next, the full IDS dataset is loaded. If a locally stored file named data.csv already exists, it is read directly to avoid long loading times. If not, the dataset is downloaded once via kagglehub. Since the original dataset is split into many individual CSV files, each file is read, concatenated into a single large DataFrame, and then saved locally. This ensures that subsequent notebook runs do not require a re-download or slow file merging operations."
   ],
   "id": "2f1472baeabfe3db"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-20T10:36:29.209343Z",
     "start_time": "2025-11-20T10:28:13.206028Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "if os.path.exists('data.csv'):\n",
    "    full_df = pd.read_csv('data.csv')\n",
    "else:\n",
    "    path = kagglehub.dataset_download(\"solarmainframe/ids-intrusion-csv\")\n",
    "    print(\"Files:\", os.listdir(path))\n",
    "    csv_files = [f for f in os.listdir(path) if f.endswith(\".csv\")]\n",
    "    dfs = {file: pd.read_csv(os.path.join(path, file), low_memory=False) for file in csv_files}\n",
    "    full_df = pd.concat(dfs.values(), ignore_index=True)\n",
    "    full_df.to_csv('data.csv', index=False)"
   ],
   "id": "24897eef353a477c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Class Distribution Analysis\n",
    "To understand the types of attacks included in the dataset, the distribution of the Label column is inspected. This analysis reveals a strong class imbalance. The “Benign” class contains over 13 million entries, while some categories, such as SQL Injection or XSS, contain only a few dozen samples. At the same time, both DoS and DDoS attacks occur in large numbers, making them suitable candidates for constructing robust reduced datasets. This observation motivates the decision to focus on these two attack types."
   ],
   "id": "ed13f58dfb85fc17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T10:36:36.187999Z",
     "start_time": "2025-11-20T10:36:35.712316Z"
    }
   },
   "cell_type": "code",
   "source": "full_df['Label'].value_counts()",
   "id": "984f38e6a9970dee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Benign                      13484708\n",
       "DDOS attack-HOIC              686012\n",
       "DDoS attacks-LOIC-HTTP        576191\n",
       "DoS attacks-Hulk              461912\n",
       "Bot                           286191\n",
       "FTP-BruteForce                193360\n",
       "SSH-Bruteforce                187589\n",
       "Infilteration                 161934\n",
       "DoS attacks-SlowHTTPTest      139890\n",
       "DoS attacks-GoldenEye          41508\n",
       "DoS attacks-Slowloris          10990\n",
       "DDOS attack-LOIC-UDP            1730\n",
       "Brute Force -Web                 611\n",
       "Brute Force -XSS                 230\n",
       "SQL Injection                     87\n",
       "Label                             59\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset Reduction\n",
    "The reduction process begins by converting all labels to lowercase to simplify matching and filtering logic. Using keywords such as “dos” and “ddos”, all corresponding attack samples are extracted and grouped together. Benign samples are collected separately, resulting in two clearly defined categories: attack traffic and normal network traffic. A binary target variable is then created, labeling attack entries as “1” and benign entries as “0”, preparing the dataset for downstream modeling."
   ],
   "id": "30a6ff49274fb092"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T11:13:45.913166Z",
     "start_time": "2025-11-20T10:36:36.202442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_df['label_lower'] = full_df['Label'].str.lower()\n",
    "dos_keywords = ['dos', 'ddos']\n",
    "\n",
    "df_attack = full_df[full_df['label_lower'].str.contains('|'.join(dos_keywords))].copy()\n",
    "df_attack['label_binar'] = 1  # Attack\n",
    "\n",
    "df_benign = full_df[full_df['Label'] == 'Benign'].copy()\n",
    "df_benign['label_binar'] = 0  # Normal\n",
    "\n",
    "total_size = 1000000\n",
    "\n",
    "def create_balanced_datasets(benign_df, attack_df, prefix):\n",
    "    size_per_class = total_size // 2\n",
    "\n",
    "    max_possible = min(len(benign_df), len(attack_df))\n",
    "    if size_per_class > max_possible:\n",
    "        print(f\"Not enough samples for total size {total_size}. Max possible per class: {max_possible}\")\n",
    "        size_per_class = max_possible\n",
    "\n",
    "    benign_sample = benign_df.sample(n=size_per_class, random_state=42)\n",
    "    attack_sample = attack_df.sample(n=size_per_class, random_state=42)\n",
    "\n",
    "    df_balanced = pd.concat([benign_sample, attack_sample])\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    filename = f\"{prefix}_{total_size}.csv\"\n",
    "    df_balanced.to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"Created {filename} with {size_per_class} benign + {size_per_class} attack samples.\")\n",
    "\n",
    "create_balanced_datasets(df_benign, df_attack, \"dos_vs_benign\")"
   ],
   "id": "da7356c1e8f87c95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Created dos_vs_benign_100000.csv with 50000 benign + 50000 attack samples.\n",
      "✔️ Created dos_vs_benign_1000000.csv with 500000 benign + 500000 attack samples.\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
